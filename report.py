from glob import glob
from jinja2 import Template
import os, sys, json
import re, time

def parse_skewer_log(log_fn):
    """Parse the log file generated by FLASH, and return the total reads and combined reads
    :param log_fn:
    :return: total, trimmed
    """
    total, trimmed = 0, 0
    with open(log_fn) as f_in:
        for line in f_in:
            if "read pairs processed" in line:
                total += [int(s) for s in line.split() if s.isdigit()][0]
            if "read pairs available; of these" in line:
                trimmed += [int(s) for s in line.split() if s.isdigit()][0]
    return total, trimmed


def parse_flash_log(log_fn):
    """Parse the log file generated by FLASH, and return the total reads and combined reads
    :param log_fn:
    :return: total, combined
    """
    total = 0
    combined = 0
    pct = 0
    with open(log_fn) as f_in:
        for line in f_in:
            if "Total reads" in line:
                total = [int(s) for s in line.split() if s.isdigit()][0]
            elif "Combined reads" in line:
                combined = [int(s) for s in line.split() if s.isdigit()][0]
            else:
                continue
    return total, combined

def parse_trim_log(log_fn):
    """
    Parse the PCR primer triming
    :param log_fn:
    :return:
    """
    total = 0
    trimmed = 0
    with open(log_fn) as f_in:
        for line in f_in:
            if "   Processed reads" in line:
                total += [int(s) for s in line.split() if s.isdigit()][0]
            elif "     Trimmed reads" in line:
                trimmed += [int(s) for s in line.split() if s.isdigit()][0]
            else:
                continue
    return total, trimmed

def parse_biom_summary(fn):
    """
    Parse the biom summary file
    :param fn: biom_summary.txt
    :return: min, mean, max, sample coverage
    """
    sample_cnt = {}
    min_cnt = 0
    median_cnt = 0
    max_cnt = 0
    with open(fn) as f_in:
        sample_line = False
        for line in f_in:
            nums =  re.findall("\d+.\d+", line)
            if "Counts/sample detail:" in line: # all line follow are sample counts
                sample_line = True
            elif   "Min:" in line:
                min_cnt =nums[0]
            elif "Max:" in line:
                max_cnt = nums[0]
            elif "Median" in line:
                median_cnt = nums[0]
            elif sample_line is True:
                fields = line.split(":")
                sampleID = fields[0].strip()
                sampleCount = fields[1]
                sample_cnt[sampleID] = sampleCount
    return min_cnt, median_cnt, max_cnt, sample_cnt

def parse_split_libraries(log_fn):
    """
    Parse log file from split_libraries_fastq.py
    :param log_fn:
    :return:
    """
    mat = {}
    with open(log_fn) as f_in:
        for line in f_in:
            if "---" in line: #end of one section
                mat[sampleID] = {"Total": total,
                                 "Too Short": short,
                                 "After": filtered,
                                 "Median length": med_len}
            elif "Total number of input sequences" in line:
                total = [int(s) for s in line.split() if s.isdigit()][0]
            elif "Read too short after quality truncation" in line:
                short = [int(s) for s in line.split() if s.isdigit()][0]
            elif "Total number seqs written" in line:
                filtered = [int(s) for s in line.split() if s.isdigit()][0]
            elif "Median sequence length" in line:
                med_len = re.findall("\d+.\d+", line)[0]
            else: # find the sample id
                fields = line.split()
                if len(fields) == 2:
                    sampleID = fields[0]
    return mat

def generate_report(config):
    outputDir= config["workdir"]
    PIname = config["PIname"]
    project = config["project"]

    ref = config["reference"]

    # collect sample information
    amplicon_stat = parse_split_libraries(os.path.join(outputDir, "logs/split_library_log.txt"))
    sample_stat = []
    for group, samples in sorted(config["group"].items()):
        for sampleID in samples:
            total_reads, _0 = parse_skewer_log(os.path.join(outputDir, "logs/trim_illumina_{}.log".format(sampleID) ))

            _0, merged_reads = parse_flash_log(os.path.join(outputDir, "logs/flash_merge_{}.log".format(sampleID) )) \
                                if os.path.exists("logs/flash_merge_{}.log".format(sampleID)) else "na"
            _0, trimmed_reads = parse_trim_log(os.path.join(outputDir, "logs/trim_primer_{}.log".format(sampleID) ))
            med_length, filtered_reads =  amplicon_stat[sampleID]["Median length"], amplicon_stat[sampleID]["After"]
            sample = dict(name=sampleID, group=group, total_reads=total_reads, merged_reads=merged_reads,
                          trimmed_reads=trimmed_reads, filtered_reads=filtered_reads, med_length=med_length)
            sample_stat.append(sample)
    # collect taxa summary plot information
    taxa_url = './taxa_summary/taxa_summary_plots/bar_charts.html'
    taxa_html = ''
    with open(taxa_url) as f_in:
        parsing = False
        for line in f_in:
            if 'usemap="#pointsrect6"' in line:
                taxa_html += line.replace("'", '"').replace('src="', 'alt="[Taxa Summary Plot]" src="./taxa_summary/taxa_summary_plots/')
                print line
                print taxa_html
                break
            # if "raw_data/otus_sorted_L6.txt" in line:
            #     parsing = True
            # if parsing:
            #     line = line.replace("'", '"')
            #     line = line.replace('a href="', 'a href="./taxa_summary/taxa_summary_plots/')
            #     line = line.replace('img src="', 'a href="./taxa_summary/taxa_summary_plots/')
            #     taxa_html += line
            #     if "</MAP>" in line:
            #         parsing = False
            #         break

    arare_utl = './arare/alpha_rarefaction_plots/rarefaction_plots.html'

    bdiv_url = glob("bdiv/*/*.html")
    bdiv_text = [u.split("/")[-2].lower().replace("emperor_", "").replace("pcoa", "PCoA").replace("_", " ") for u in bdiv_url]

    heatmap_url = './heatmap/otus_heatmap.png'
    tree_url = './otus/rep_set.tre'

    t = Template(open(os.path.join(os.path.abspath(sys.path[0]), "report.html")).read())
    html_str =  t.render(PIname = PIname, description = project, time = time.asctime(), ref=config["reference"],
                                    samples = sample_stat, sample_names = ", ".join([s["name"] for s in sample_stat]),
                                    taxa_url = taxa_url,
                                    taxa_html = taxa_html,
                                    arare_utl = arare_utl,
                                    bdiv_text_url = zip(bdiv_text,bdiv_url),
                                    heatmap_url = heatmap_url,
                                    tree_url = tree_url)
    return html_str


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("USAGE: {} <run_config>".format(sys.argv[0]))
        exit()
    config = json.load(open(sys.argv[1]))
    print "Generating report..."
    r = generate_report(config)
    with open(os.path.join(config["workdir"], "report.html"), 'w') as html_out:
        html_out.write(r)

    print "Report Finished"